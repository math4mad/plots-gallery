Entity,Code,Year,MMLU avg,Training compute (petaFLOP),Training dataset size,Organisation
BLOOM,,2022,39.13,412000000,390000000000,"HuggingFace, BigScience"
BloombergGPT,,2023,39.18,212000000,708000000000,Bloomberg
Chinchilla,,2022,67.5,588000000,1400000000000,DeepMind
GLM-130B,,2022,44.8,312000000,400000000000,Tsinghua KEG
GPT-2 (finetuned),,2019,32.4,36000,4000000000,OpenAI
GPT-3 (davinci),,2020,43.9,393000000,374000000000,OpenAI
GPT-3.5,,2022,70,393000000,374000000000,OpenAI
GPT-4,,2023,86.4,20200000000,26100000000000,OpenAI
GPT-NeoX-20B,,2022,33.6,21200000,177000000000,Eleuther
Gopher (0.4B),,2021,25.7,751000,300000000000,DeepMind
Gopher (1.4B),,2021,27.3,2520000,300000000000,DeepMind
Gopher (280B),,2021,60,504000000,300000000000,DeepMind
Gopher (7B),,2021,29.5,12800000,300000000000,DeepMind
LLaMA (13B),,2023,46.9,78000000,1000000000000,Meta AI
LLaMA (33B),,2023,57.8,273000000,1400000000000,Meta AI
LLaMA (65B),,2023,63.4,548000000,1400000000000,Meta AI
LLaMA (7B),,2023,35.1,40200000,1000000000000,Meta AI
OPT,,2022,35.99,172000000,434000000000,Meta AI
PaLM (540B),,2022,69.3,2530000000,780000000000,Google Research
PaLM (62B),,2022,53.7,296000000,795000000000,Google Research
PaLM (62B+),,2022,62.8,493000000,1330000000000,Google Research
PaLM (8B),,2022,25.3,37400000,780000000000,Google Research
PaLM-2,,2023,78.3,8160000000,4000000000000,Google Research
code-davinci-002,,2022,68.3,393000000,374000000000,OpenAI